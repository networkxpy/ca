################### tfidf #######################
import numpy as np
import pandas as pd

# Sample documents
documents = [
    "the cat in the hat",
    "the quick brown fox",
    "the fox jumps over the lazy dog",
    "never jump over the lazy dog quickly"
]

# Define the keywords (terms of interest)
keywords = ["the", "fox", "dog", "lazy", "quick"]

# Calculate Term Frequency (TF)
tf = []
for doc_count, doc in enumerate(documents):
    print("Term Frequency in Document", doc_count+1)
    tf_series = pd.Series(doc.split()).value_counts()
    tf.append(tf_series)
    print(tf_series)
    print()

# Calculate Inverse Document Frequency (IDF)
idf = {}
for term in keywords:
    idf[term] = np.log(len(documents)/(sum([1 for doc in documents if term in doc])+1))
    print("IDF of {} :".format(term))
    print(idf[term])
    print()

# Calculate TF-IDF
print("TF-IDF Scores:")
tf_idf = []
for doc_count, tf_series in enumerate(tf):
    tf_idf_dict = {}
    for term in keywords:
        tf_idf_dict[term] = tf_series.get(term, 0) * idf[term]
    tf_idf.append(tf_idf_dict)
    print("Document", doc_count+1)
    print(tf_idf_dict)
    print()

############# TF IDF Inbuild ##################
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Sample documents
documents = [
    "the cat in the hat",
    "the quick brown fox",
    "the fox jumps over the lazy dog",
    "never jump over the lazy dog quickly"
]

# Create a TfidfVectorizer object
vectorizer = TfidfVectorizer(vocabulary=["the", "fox", "dog", "lazy", "quick"])

# Fit the documents and transform them into TF-IDF matrix
tfidf_matrix = vectorizer.fit_transform(documents)

# Convert the TF-IDF matrix to a DataFrame for better readability
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())

# Display the TF-IDF scores
print("TF-IDF Scores:")
print(tfidf_df)

################# Preprocessing ##########################
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

#Loading the documents
documents = open("documents.txt")
content = documents.readlines()
print(content[:5])

#Grouping them into paragraphs
paragraphs = []
temp = ""
for i in content:
  if i=='\n':
    paragraphs.append(temp)
    temp = ""
  else:
    temp += i

paragraphs[:2]

#Basic Text Preprocessing 1. Remove Special Characters 2. Lowercasing
def preprocess(text):
  text = re.sub('[^A-Za-z0-9]+   ', '', text)
  text = text.lower()
  text = text.replace("\n"," ")
  text = text.replace("\ufeff","")
  return text

preprocessed_paragraphs = []
for i in paragraphs:
  preprocessed_paragraphs.append(preprocess(i))

preprocessed_paragraphs[:2]

############################# Inverted index #################
#Loading the keywords
keywords = open("keywords.txt")
keywords = keywords.readlines()
keywords = [i.replace("\n","") for i in keywords]
keywords[:3]

#Inverted Index: {keyword:[list of documents containing keyword]}
inverted_index = {}
for keyword in keywords:
  inverted_index[keyword] = []

for keyword in keywords:
  for i_doc in range(len(preprocessed_paragraphs)):
    if keyword in preprocessed_paragraphs[i_doc]:
      inverted_index[keyword].append(i_doc)

print(inverted_index)


################## Boolean queries ###################################
#Converting preprocessed paragraphs into binary bag of words
vectorizer = CountVectorizer()
binary_bog = vectorizer.fit_transform(preprocessed_paragraphs)
binary_bog_values  = binary_bog.toarray()
features_bog = list(vectorizer.get_feature_names_out())

df_bog  = pd.DataFrame(binary_bog_values,columns=features_bog)
df_bog.head(2)



#Binary Query using And Or
query = "reasoning AND home OR group"
words = query.split()
res_df = None
character = 0
while character < len(words):

  if words[character]=="OR":
    res_df = res_df | df_bog[words[character+1]]
    character += 2
  elif words[character]=="AND":
    res_df = res_df & df_bog[words[character+1]]
    character += 2
  else:
    res_df = df_bog[words[character]]
    character += 1

res_df = list(res_df)

#List of documents satisfying the given query
documents_index = []
for i in range(len(res_df)):
  if res_df[i]>0:
    documents_index.append(i)
  else:
    continue
print("Satisfied Results Document Index:",documents_index)
print("Query:",query)
print("Results:")
for i in range(len(documents_index[:5])):
  print("TOP ",i+1,":",paragraphs[documents_index[i]])



################## Ranked queries ########################

def cosine_distance(v1,v2):
  return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))

def retriever(content,query,top_k):
  result = {}
  for i in range(len(content)):
    result[i] = cosine_distance(content[i],query)

  result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1],reverse=True)}
  return list(result.keys())[:top_k]

vectorizer = TfidfVectorizer()
tfidf_para = vectorizer.fit_transform(preprocessed_paragraphs).toarray()


query = ["international machine learning conference".lower()]
tfidf_query = list(vectorizer.transform(query).toarray()[0])
top_k = 5

results = retriever(tfidf_para,tfidf_query,top_k)
results


for i in range(len(results)):
  print("TOP ",i+1,":",paragraphs[results[i]])
