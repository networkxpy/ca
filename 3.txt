################### tfidf #######################
import numpy as np
import pandas as pd

# Sample documents
documents = [
    "the cat in the hat",
    "the quick brown fox",
    "the fox jumps over the lazy dog",
    "never jump over the lazy dog quickly"
]

# Define the keywords (terms of interest)
keywords = ["the", "fox", "dog", "lazy", "quick"]

# Calculate Term Frequency (TF)
tf = []
for doc_count, doc in enumerate(documents):
    print("Term Frequency in Document", doc_count+1)
    tf_series = pd.Series(doc.split()).value_counts()
    tf.append(tf_series)
    print(tf_series)
    print()

# Calculate Inverse Document Frequency (IDF)
idf = {}
for term in keywords:
    idf[term] = np.log(len(documents)/(sum([1 for doc in documents if term in doc])+1))
    print("IDF of {} :".format(term))
    print(idf[term])
    print()

# Calculate TF-IDF
print("TF-IDF Scores:")
tf_idf = []
for doc_count, tf_series in enumerate(tf):
    tf_idf_dict = {}
    for term in keywords:
        tf_idf_dict[term] = tf_series.get(term, 0) * idf[term]
    tf_idf.append(tf_idf_dict)
    print("Document", doc_count+1)
    print(tf_idf_dict)
    print()

############# TF IDF Inbuild ##################
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Sample documents
documents = [
    "the cat in the hat",
    "the quick brown fox",
    "the fox jumps over the lazy dog",
    "never jump over the lazy dog quickly"
]

# Create a TfidfVectorizer object
vectorizer = TfidfVectorizer(vocabulary=["the", "fox", "dog", "lazy", "quick"])

# Fit the documents and transform them into TF-IDF matrix
tfidf_matrix = vectorizer.fit_transform(documents)

# Convert the TF-IDF matrix to a DataFrame for better readability
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())

# Display the TF-IDF scores
print("TF-IDF Scores:")
print(tfidf_df)

################# Preprocessing ##########################
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

#Loading the documents
documents = open("documents.txt")
content = documents.readlines()
print(content[:5])

#Grouping them into paragraphs
paragraphs = []
temp = ""
for i in content:
  if i=='\n':
    paragraphs.append(temp)
    temp = ""
  else:
    temp += i

paragraphs[:2]

#Basic Text Preprocessing 1. Remove Special Characters 2. Lowercasing
def preprocess(text):
  text = re.sub('[^A-Za-z0-9]+   ', '', text)
  text = text.lower()
  text = text.replace("\n"," ")
  text = text.replace("\ufeff","")
  return text

preprocessed_paragraphs = []
for i in paragraphs:
  preprocessed_paragraphs.append(preprocess(i))

preprocessed_paragraphs[:2]

############################# Inverted index #################
#Loading the keywords
keywords = open("keywords.txt")
keywords = keywords.readlines()
keywords = [i.replace("\n","") for i in keywords]
keywords[:3]

#Inverted Index: {keyword:[list of documents containing keyword]}
inverted_index = {}
for keyword in keywords:
  inverted_index[keyword] = []

for keyword in keywords:
  for i_doc in range(len(preprocessed_paragraphs)):
    if keyword in preprocessed_paragraphs[i_doc]:
      inverted_index[keyword].append(i_doc)

print(inverted_index)


################## Boolean queries ###################################
#Converting preprocessed paragraphs into binary bag of words
vectorizer = CountVectorizer()
binary_bog = vectorizer.fit_transform(preprocessed_paragraphs)
binary_bog_values  = binary_bog.toarray()
features_bog = list(vectorizer.get_feature_names_out())

df_bog  = pd.DataFrame(binary_bog_values,columns=features_bog)
df_bog.head(2)



#Binary Query using And Or
query = "reasoning AND home OR group"
words = query.split()
res_df = None
character = 0
while character < len(words):

  if words[character]=="OR":
    res_df = res_df | df_bog[words[character+1]]
    character += 2
  elif words[character]=="AND":
    res_df = res_df & df_bog[words[character+1]]
    character += 2
  else:
    res_df = df_bog[words[character]]
    character += 1

res_df = list(res_df)

#List of documents satisfying the given query
documents_index = []
for i in range(len(res_df)):
  if res_df[i]>0:
    documents_index.append(i)
  else:
    continue
print("Satisfied Results Document Index:",documents_index)
print("Query:",query)
print("Results:")
for i in range(len(documents_index[:5])):
  print("TOP ",i+1,":",paragraphs[documents_index[i]])



################## Ranked queries ########################

def cosine_distance(v1,v2):
  return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))

def retriever(content,query,top_k):
  result = {}
  for i in range(len(content)):
    result[i] = cosine_distance(content[i],query)

  result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1],reverse=True)}
  return list(result.keys())[:top_k]

vectorizer = TfidfVectorizer()
tfidf_para = vectorizer.fit_transform(preprocessed_paragraphs).toarray()


query = ["international machine learning conference".lower()]
tfidf_query = list(vectorizer.transform(query).toarray()[0])
top_k = 5

results = retriever(tfidf_para,tfidf_query,top_k)
results


for i in range(len(results)):
  print("TOP ",i+1,":",paragraphs[results[i]])
################################## Topic modelling ###############################
import gensim
import gensim.corpora as corpora
from gensim.models import LdaModel
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

# Download required NLTK data files
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Sample documents
documents = [
    "Artificial intelligence and machine learning are transforming industries.",
    "Deep learning techniques are being used in many AI applications.",
    "Natural language processing is a crucial area of AI.",
    "Big data analytics involves processing large amounts of information.",
    "Data science and AI are closely related fields."
]

# Preprocess the documents
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    tokens = word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]
    return tokens

# Preprocess all documents
texts = [preprocess(doc) for doc in documents]

# Create a dictionary and corpus needed for LDA
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Build the LDA model
num_topics = 2  # Number of topics
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)

# Display the topics
topics = lda_model.print_topics(num_words=4)
for idx, topic in topics:
    print(f"Topic {idx+1}: {topic}")
############################################## classification ####################################
# Step 1: Import Necessary Libraries
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import gensim
import gensim.corpora as corpora
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Step 2: Load the Dataset
url = 'https://figshare.com/ndownloader/files/26202692'
df = pd.read_csv(url)

# Step 3: Data Exploration
print(df.head())  # Inspect the first few rows
print(df['label'].value_counts())  # Check class distribution

# Step 4: Text Preprocessing
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    tokens = word_tokenize(text.lower())  # Tokenize text
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]  # Remove stopwords and lemmatize
    return ' '.join(tokens)

# Apply preprocessing to the 'text' column
df['cleaned_text'] = df['text'].apply(preprocess)

# Step 5: Topic Modeling with LDA
texts = [text.split() for text in df['cleaned_text']]  # Prepare texts for LDA
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, random_state=42)

# Print the topics discovered by LDA
for idx, topic in lda_model.print_topics(num_words=5):
    print(f"Topic {idx+1}: {topic}")

# Infer topics for each document
df['topics'] = [lda_model[dictionary.doc2bow(text)] for text in texts]

# Step 6: Feature Extraction using TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf.fit_transform(df['cleaned_text'])

# Convert LDA topics to a feature matrix
import numpy as np
X_topics = np.array([[topic[1] for topic in doc] for doc in df['topics']])
X = np.hstack((X_tfidf.toarray(), X_topics))  # Combine TF-IDF and topic features

y = df['label']  # Target labels

# Step 7: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 8: Apply Classification Algorithms
# Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression Performance:")
print(classification_report(y_test, y_pred_lr))

# Support Vector Machine (SVM)
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)
print("SVM Performance:")
print(classification_report(y_test, y_pred_svm))

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest Performance:")
print(classification_report(y_test, y_pred_rf))

# Step 9: Performance Evaluation
# Accuracy of each model
print(f"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr)}")
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm)}")
print(f"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf)}")

# Step 10 (Optional): Semantic Analysis with Word2Vec
# Uncomment this part if you want to use word embeddings instead of TF-IDF
# from gensim.models import Word2Vec
# w2v_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=5, workers=4)
# w2v_vectors = np.array([np.mean([w2v_model.wv[word] for word in doc if word in w2v_model.wv] or [np.zeros(100)], axis=0) for doc in texts])

# Continue classification using w2v_vectors as features
